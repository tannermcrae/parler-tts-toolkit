# Build stage for dependencies
FROM pytorch/pytorch:2.5.1-cuda12.4-cudnn9-runtime AS builder

USER root
RUN apt-get update && apt-get install -y git

WORKDIR /opt/program
COPY training/requirements.txt .
RUN pip install -r requirements.txt


# Final stage
FROM builder

# Add GPU verification label
LABEL com.amazonaws.sagemaker.inference.cuda.verified_versions=12.4

# Copy config (this will be a separate layer that changes more frequently)
COPY training/ training/

# Copy the src. The directory has src/parler_tts so this will create the right structure.
COPY src/ src/

COPY inference/ inference/

# Make the training script executable
RUN chmod +x training/entrypoint.sh

# Make GPUs visibile
ENV NVIDIA_VISIBLE_DEVICES=all
ENV CUDA_VISIBLE_DEVICES=all

# Set python path so the model is executable
ENV PYTHONPATH=/opt/program

EXPOSE 8080

# SageMaker always runs the container like docker image serve so we need to handle that in the script.
ENTRYPOINT ["/opt/program/training/entrypoint.sh"]